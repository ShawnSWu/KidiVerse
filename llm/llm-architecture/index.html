<!doctype html><html lang=zh-TW class=scroll-smooth><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>KidiVerse |</title><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet><link rel=stylesheet href=/KidiVerse/css/style.css><link rel=icon type=image/svg+xml href=/KidiVerse/images/logo.svg></head><body><div class=page-container><div class=page-content><aside class=sidebar><aside class=sidebar><div class=sidebar-container><header class=sidebar-header><h2 class=sidebar-title><a href=https://shawnswu.github.io/KidiVerse/ class=sidebar-home-link>KidiVerse</a></h2></header><nav class=sidebar-nav><div class=sidebar-folder data-folder-id=section-0><button class=sidebar-folder-toggle aria-expanded=false>
<svg class="folder-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M22 19a2 2 0 01-2 2H4a2 2 0 01-2-2V5a2 2 0 012-2h5l2 3h9a2 2 0 012 2z"/></svg>
<span class=folder-title>Blockchain</span>
<svg class="chevron" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polyline points="9 18 15 12 9 6"/></svg></button><div class=sidebar-folder-content><ul class=sidebar-sublist><li class=sidebar-item><a href=/KidiVerse/blockchain/blockchain-overview/ class=sidebar-link><svg class="file-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M13 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V9z"/><polyline points="13 2 13 9 20 9"/></svg>
<span class=link-text>Blockchain Overview</span></a></li><li class=sidebar-item><a href=/KidiVerse/blockchain/consensus/ class=sidebar-link><svg class="file-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M13 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V9z"/><polyline points="13 2 13 9 20 9"/></svg>
<span class=link-text>Consensus</span></a></li><li class=sidebar-item><a href=/KidiVerse/blockchain/smart-contracts/ class=sidebar-link><svg class="file-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M13 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V9z"/><polyline points="13 2 13 9 20 9"/></svg>
<span class=link-text>Smart Contracts</span></a></li></ul></div></div><div class=sidebar-folder data-folder-id=section-1><button class=sidebar-folder-toggle aria-expanded=false>
<svg class="folder-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M22 19a2 2 0 01-2 2H4a2 2 0 01-2-2V5a2 2 0 012-2h5l2 3h9a2 2 0 012 2z"/></svg>
<span class=folder-title>Container</span>
<svg class="chevron" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polyline points="9 18 15 12 9 6"/></svg></button><div class=sidebar-folder-content><ul class=sidebar-sublist><li class=sidebar-item><a href=/KidiVerse/container/docker/ class=sidebar-link><svg class="file-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M13 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V9z"/><polyline points="13 2 13 9 20 9"/></svg>
<span class=link-text>Docker</span></a></li><li class=sidebar-item><div class=sidebar-folder data-folder-id=folder-feaafff89ba8906cbe1e4006a613aca9><button class=sidebar-folder-toggle aria-expanded=false>
<svg class="folder-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M22 19a2 2 0 01-2 2H4a2 2 0 01-2-2V5a2 2 0 012-2h5l2 3h9a2 2 0 012 2z"/></svg>
<span class=folder-title>Kubernetes</span>
<svg class="chevron" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polyline points="9 18 15 12 9 6"/></svg></button><div class=sidebar-folder-content><ul class=sidebar-sublist><li class=sidebar-item><a href=/KidiVerse/container/kubernetes/controller/ class=sidebar-link><svg class="file-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M13 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V9z"/><polyline points="13 2 13 9 20 9"/></svg>
<span class=link-text>Controller</span></a></li><li class=sidebar-item><a href=/KidiVerse/container/kubernetes/ingress/ class=sidebar-link><svg class="file-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M13 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V9z"/><polyline points="13 2 13 9 20 9"/></svg>
<span class=link-text>Ingress</span></a></li><li class=sidebar-item><a href=/KidiVerse/container/kubernetes/pod/ class=sidebar-link><svg class="file-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M13 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V9z"/><polyline points="13 2 13 9 20 9"/></svg>
<span class=link-text>Pod</span></a></li></ul></div></div></li></ul></div></div><div class=sidebar-folder data-folder-id=section-2><button class=sidebar-folder-toggle aria-expanded=false>
<svg class="folder-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M22 19a2 2 0 01-2 2H4a2 2 0 01-2-2V5a2 2 0 012-2h5l2 3h9a2 2 0 012 2z"/></svg>
<span class=folder-title>Git</span>
<svg class="chevron" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polyline points="9 18 15 12 9 6"/></svg></button><div class=sidebar-folder-content><ul class=sidebar-sublist><li class=sidebar-item><a href=/KidiVerse/git/git-basics/ class=sidebar-link><svg class="file-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M13 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V9z"/><polyline points="13 2 13 9 20 9"/></svg>
<span class=link-text>Git Basics</span></a></li></ul></div></div><div class=sidebar-folder data-folder-id=section-3><button class=sidebar-folder-toggle aria-expanded=false>
<svg class="folder-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M22 19a2 2 0 01-2 2H4a2 2 0 01-2-2V5a2 2 0 012-2h5l2 3h9a2 2 0 012 2z"/></svg>
<span class=folder-title>GRPC vs GraphQL</span>
<svg class="chevron" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polyline points="9 18 15 12 9 6"/></svg></button><div class=sidebar-folder-content><ul class=sidebar-sublist><li class=sidebar-item><a href=/KidiVerse/grpc-vs-graphql/graphql/ class=sidebar-link><svg class="file-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M13 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V9z"/><polyline points="13 2 13 9 20 9"/></svg>
<span class=link-text>GraphQL</span></a></li><li class=sidebar-item><a href=/KidiVerse/grpc-vs-graphql/grpc/ class=sidebar-link><svg class="file-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M13 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V9z"/><polyline points="13 2 13 9 20 9"/></svg>
<span class=link-text>GRPC</span></a></li></ul></div></div><div class=sidebar-folder data-folder-id=section-4><button class=sidebar-folder-toggle aria-expanded=false>
<svg class="folder-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M22 19a2 2 0 01-2 2H4a2 2 0 01-2-2V5a2 2 0 012-2h5l2 3h9a2 2 0 012 2z"/></svg>
<span class=folder-title>Kafka</span>
<svg class="chevron" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polyline points="9 18 15 12 9 6"/></svg></button><div class=sidebar-folder-content><ul class=sidebar-sublist><li class=sidebar-item><a href=/KidiVerse/kafka/broker/ class=sidebar-link><svg class="file-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M13 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V9z"/><polyline points="13 2 13 9 20 9"/></svg>
<span class=link-text>Broker</span></a></li><li class=sidebar-item><a href=/KidiVerse/kafka/consumer/ class=sidebar-link><svg class="file-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M13 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V9z"/><polyline points="13 2 13 9 20 9"/></svg>
<span class=link-text>Consumer</span></a></li><li class=sidebar-item><a href=/KidiVerse/kafka/kafka-streams/ class=sidebar-link><svg class="file-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M13 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V9z"/><polyline points="13 2 13 9 20 9"/></svg>
<span class=link-text>Kafka Streams</span></a></li><li class=sidebar-item><a href=/KidiVerse/kafka/kafka/ class=sidebar-link><svg class="file-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M13 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V9z"/><polyline points="13 2 13 9 20 9"/></svg>
<span class=link-text>Kafka</span></a></li><li class=sidebar-item><a href=/KidiVerse/kafka/producers/ class=sidebar-link><svg class="file-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M13 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V9z"/><polyline points="13 2 13 9 20 9"/></svg>
<span class=link-text>Producers</span></a></li></ul></div></div><div class=sidebar-folder data-folder-id=section-5><button class=sidebar-folder-toggle aria-expanded=false>
<svg class="folder-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M22 19a2 2 0 01-2 2H4a2 2 0 01-2-2V5a2 2 0 012-2h5l2 3h9a2 2 0 012 2z"/></svg>
<span class=folder-title>LLM</span>
<svg class="chevron" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polyline points="9 18 15 12 9 6"/></svg></button><div class=sidebar-folder-content><ul class=sidebar-sublist><li class=sidebar-item><a href=/KidiVerse/llm/fine-tuning/ class=sidebar-link><svg class="file-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M13 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V9z"/><polyline points="13 2 13 9 20 9"/></svg>
<span class=link-text>Fine Tuning</span></a></li><li class=sidebar-item><a href=/KidiVerse/llm/llm-architecture/ class="sidebar-link active"><svg class="file-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M13 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V9z"/><polyline points="13 2 13 9 20 9"/></svg>
<span class=link-text>Llm Architecture</span></a></li><li class=sidebar-item><a href=/KidiVerse/llm/prompt-engineering/ class=sidebar-link><svg class="file-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M13 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V9z"/><polyline points="13 2 13 9 20 9"/></svg>
<span class=link-text>Prompt Engineering</span></a></li><li class=sidebar-item><a href=/KidiVerse/llm/rag_guide/ class=sidebar-link><svg class="file-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M13 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V9z"/><polyline points="13 2 13 9 20 9"/></svg>
<span class=link-text>Rag Guide</span></a></li></ul></div></div></nav><div class=sidebar-footer></div></div></aside></aside><main class=main-content><article class=post-article><div class=markdown-area><h1 id=large-language-models>Large Language Models</h1><p>These notes consolidate three key areas of modern large‑language models (LLMs):</p><ol><li><strong>Architecture & Transformer Basics</strong></li><li><strong>Training, Fine‑Tuning, and Retrieval‑Augmented Generation (RAG)</strong></li><li><strong>Inference, Serving, and Optimization</strong></li></ol><hr><h2 id=1llm-architecture--transformer-basics>1 LLM Architecture & Transformer Basics</h2><h3 id=11transformer-blueprint>1.1 Transformer Blueprint</h3><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph TD
  A[Input Tokens] --&gt;|Embedding| B
  B --&gt; C[Positional Encoding]
  C --&gt; D[Transformer Block × N]
  D --&gt; E[Linear + Softmax]
</code></pre><p><strong>Self‑Attention</strong></p><p>For head <em>h</em>:</p><p>[
\mathrm{Attention}(Q_h,K_h,V_h)=\mathrm{softmax}!\left(\frac{Q_h K_h^\top}{\sqrt{d_k}}\right)V_h
]</p><p><em>RoPE</em> and <em>ALiBi</em> add relative positioning so models extrapolate to longer contexts.</p><h3 id=12scaling-laws>1.2 Scaling Laws</h3><p>[
\text{loss}(N,D,C) \approx A N^{-\alpha} + B D^{-\beta} + C R^{-\gamma}
]</p><blockquote><p><strong>Takeaway:</strong> Bigger <em>and</em> better data lower perplexity predictably—handy for budgeting.</p></blockquote><h3 id=13parameter-landmarks-20232025>1.3 Parameter Landmarks (2023‑2025)</h3><table><thead><tr><th>Model</th><th>Params</th><th>Context</th><th>Notable Feature</th></tr></thead><tbody><tr><td>GPT‑3.5</td><td>175 B</td><td>8 k</td><td>Dense</td></tr><tr><td>Llama‑3 70 B</td><td>70 B</td><td>16 k</td><td>Open weights</td></tr><tr><td>Falcon 180 B</td><td>180 B</td><td>4 k</td><td>Arabic + code</td></tr><tr><td>Mixtral 8×22 B</td><td>45 B <em>active</em></td><td>32 k</td><td>Sparse MoE</td></tr></tbody></table><p><em>MoE = Mixture of Experts → higher capacity with lower inference cost.</em></p><hr><h2 id=2training-finetuning--rag>2 Training, Fine‑Tuning & RAG</h2><h3 id=21pretraining>2.1 Pre‑Training</h3><pre tabindex=0><code>Objective: p(token_t | token_&lt;t)
Loss:      cross‑entropy
Data:      multi‑trillion‑token mix (web, books, code)
Budget:    10³–10⁴ GPU‑days
</code></pre><ul><li><strong>Curriculum:</strong> Blend code at ~30 % to boost reasoning.</li><li><strong>Tokenizer:</strong> BPE/Unigram, 32 k–100 k vocab, avoid <code>&lt;UNK></code>.</li></ul><h3 id=22instruction-finetuning-lora-example>2.2 Instruction Fine‑Tuning (LoRA example)</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoModelForCausalLM, AutoTokenizer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> peft <span style=color:#f92672>import</span> LoraConfig, get_peft_model
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>base <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;meta-llama3-8b&#34;</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> AutoModelForCausalLM<span style=color:#f92672>.</span>from_pretrained(base, device_map<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;auto&#34;</span>)
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(base)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>peft_cfg <span style=color:#f92672>=</span> LoraConfig(r<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>, lora_alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>, target_modules<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;q_proj&#34;</span>,<span style=color:#e6db74>&#34;v_proj&#34;</span>])
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> get_peft_model(model, peft_cfg)
</span></span><span style=display:flex><span><span style=color:#75715e># ...load dataset and train...</span>
</span></span></code></pre></div><ul><li>Tune &lt; 1 % of weights → fits on a single 24 GB GPU.</li><li><strong>DPO</strong> / <strong>RLHF</strong> refine style & safety via preference data.</li></ul><h3 id=23retrievalaugmented-generation-rag>2.3 Retrieval‑Augmented Generation (RAG)</h3><pre tabindex=0><code class=language-mermaid data-lang=mermaid>flowchart LR
  Q[User Query] --&gt; E[Embedder]
  E --&gt; V[Vector DB]
  V --&gt;|Top‑k| C[Context]
  Q --&gt; M[LLM]
  C --&gt; M
</code></pre><ol><li>Embed query & docs.</li><li>Retrieve top‑k passages.</li><li>Prepend to prompt (chunk 512–1024 tokens, 20 % overlap).</li></ol><p>Popular stacks: <strong>LangChain / LlamaIndex</strong> + <strong>Qdrant / Pinecone</strong>.</p><h3 id=24evaluation-cheatsheet>2.4 Evaluation Cheat‑Sheet</h3><table><thead><tr><th>Metric</th><th>Checks…</th></tr></thead><tbody><tr><td><strong>Perplexity</strong></td><td>Language modeling fit</td></tr><tr><td><strong>MMLU / GSM8K</strong></td><td>Reasoning & QA</td></tr><tr><td><strong>TruthfulQA</strong></td><td>Hallucination rate</td></tr><tr><td><strong>MT‑Bench</strong></td><td>Chat alignment</td></tr></tbody></table><hr><h2 id=3inference-serving--optimization>3 Inference, Serving & Optimization</h2><h3 id=31singlegpu-quickstart-vllm>3.1 Single‑GPU Quickstart (vLLM)</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install vllm<span style=color:#f92672>==</span>0.4.2
</span></span><span style=display:flex><span>python -m vllm.entrypoints.api_server   --model meta-llama3-8b   --max-model-len <span style=color:#ae81ff>8192</span>
</span></span></code></pre></div><p><em>vLLM</em> uses PagedAttention → hundreds of concurrent streams on one GPU.</p><h3 id=32scaling-strategies>3.2 Scaling Strategies</h3><table><thead><tr><th>Strategy</th><th>Best for…</th><th>Tools</th></tr></thead><tbody><tr><td>Tensor Parallelism</td><td>Very large dense models</td><td>Megatron‑LM, DS</td></tr><tr><td>Pipeline Parallel</td><td>Long sequences</td><td>DeepSpeed, DS</td></tr><tr><td>Sparse MoE Routing</td><td>High throughput</td><td>DeepSpeed‑MoE</td></tr><tr><td>KV‑Cache Offload</td><td>>32 k context</td><td>vLLM, SGLang</td></tr></tbody></table><h3 id=33quantization-snapshot>3.3 Quantization Snapshot</h3><table><thead><tr><th>Method</th><th>Memory Cut</th><th>Quality Loss</th></tr></thead><tbody><tr><td>INT8</td><td>4×</td><td>&lt; 1 %</td></tr><tr><td>NF4</td><td>8×</td><td>~1.5 %</td></tr><tr><td>GPTQ</td><td>4×</td><td>tunable</td></tr><tr><td>AWQ</td><td>4–6×</td><td>minimal</td></tr></tbody></table><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> awq <span style=color:#f92672>import</span> AutoAWQForCausalLM
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> AutoAWQForCausalLM<span style=color:#f92672>.</span>from_quantized(<span style=color:#e6db74>&#34;mistral-7b-awq&#34;</span>, w_bit<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>)
</span></span></code></pre></div><h3 id=34production-tips>3.4 Production Tips</h3><ul><li><strong>KServe</strong> / <strong>Ray Serve</strong> for K8s autoscaling & canaries.</li><li><strong>Prompt caching</strong> saves 20–50 % tokens.</li><li>Track <strong>Tokens/s</strong>, <strong>P95 latency</strong>, <strong>Failed requests</strong> via Prometheus.</li></ul><blockquote><p><strong>Golden Rule:</strong> If you can’t measure it, you can’t scale it.</p></blockquote><hr><h2 id=references>References</h2><ul><li>Vaswani et al., <em>Attention Is All You Need</em> (2017)</li><li>Kaplan et al., <em>Scaling Laws</em> (2020)</li><li>Fedus et al., <em>Pathways & Sparse MoE</em> (2022)</li><li>vLLM docs – <a href=https://vllm.ai/>https://vllm.ai/</a></li></ul></div></article></main></div></div><script src=/KidiVerse/js/main.js defer></script><script src=/KidiVerse/js/cosmic-background.js defer></script><script src=/KidiVerse/js/obsidian-images.js defer></script></body></html>